{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Learning is Robust to Massive Label Noise](https://arxiv.org/pdf/1705.10694)\n",
    "\n",
    "\n",
    "The paper shows that neural networks can generalize when large numbers of (non-adversarially) incorrectly labeled examples are added to datasets (MNIST, CIFAR, and ImageNet).\n",
    "\n",
    "We'll focus on uniform label noise (Experiment 1) and the MNIST dataset for computational reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, itertools, time\n",
    "\n",
    "os.makedirs('logs', exist_ok = True)\n",
    "os.makedirs('models', exist_ok = True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    ('mps' if torch.backends.mps.is_available() else\n",
    "    'cpu')\n",
    ")\n",
    "# device = 'cpu' # faster for the small models we are using\n",
    "\n",
    "def eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n",
    "    # Returns loss and accuracy of the model on the test set\n",
    "    model.eval()\n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, pred = torch.max(model(images), 1)\n",
    "            correct += (pred == labels).float().sum().item()\n",
    "            loss += criterion(model(images), labels).item()\n",
    "    return loss / len(test.dataset), correct / len(test.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLabelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Adds alpha uniformly noisy labels for every example in the original dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, alpha):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def is_noisy(self, idx):\n",
    "        return idx % (self.alpha + 1) != 0\n",
    "\n",
    "    def __len__(self):\n",
    "        n = len(self.dataset)\n",
    "        return n + (self.alpha * n)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx // (self.alpha + 1)]\n",
    "        if self.is_noisy(idx):\n",
    "            y = np.random.choice(len(self.dataset.classes))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, lr = 0.01, patience = 3, max_epochs = 100, verbose = False):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr = lr)\n",
    "\n",
    "    log = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    best_val_acc = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_acc = eval_model(model, val_loader)\n",
    "        log['train_loss'].append(loss.item())\n",
    "        log['val_loss'].append(val_loss)\n",
    "        log['val_acc'].append(val_acc)\n",
    "\n",
    "        if verbose: print(', '.join([f'Epoch {epoch + 1}'] + [f'{k}: {v[-1]:.4f}' for k, v in log.items()]))\n",
    "\n",
    "        if val_acc < best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        # Early stopping: stop if val acc has not increased in the last `patience` epochs\n",
    "        if epoch > patience and val_acc <= max(log['val_acc'][-patience-1:-1]): break \n",
    "    \n",
    "    if best_model: model.load_state_dict(best_model)\n",
    "    return model, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
    "alphas = range(0, 125, 25)\n",
    "\n",
    "lin_relu = lambda n_in, n_out: nn.Sequential(nn.Linear(n_in, n_out), nn.ReLU())\n",
    "models = {\n",
    "    'perceptron':nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)),\n",
    "    'MLP1':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), nn.Linear(256, 10)),\n",
    "    'MLP2':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), nn.Linear(128, 10)),\n",
    "    'MLP4':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), lin_relu(128, 64), nn.Linear(64, 10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST('data', download = True, train = True,   transform = transform)\n",
    "test_dataset =  datasets.MNIST('data', download = True, train = False,  transform = transform)\n",
    "\n",
    "noisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = 5)\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (0.2, 0.8), generator = torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60000\n",
      "Epoch 1, train_loss: 1.2487, val_loss: 0.0103, val_acc: 0.7740\n",
      "Epoch 2, train_loss: 0.8007, val_loss: 0.0069, val_acc: 0.8380\n",
      "Epoch 3, train_loss: 0.7382, val_loss: 0.0053, val_acc: 0.8675\n",
      "Epoch 4, train_loss: 0.6066, val_loss: 0.0044, val_acc: 0.8790\n",
      "Epoch 5, train_loss: 0.7068, val_loss: 0.0039, val_acc: 0.8860\n",
      "Epoch 6, train_loss: 0.3720, val_loss: 0.0036, val_acc: 0.8940\n",
      "Epoch 7, train_loss: 0.5438, val_loss: 0.0033, val_acc: 0.9000\n",
      "Epoch 8, train_loss: 0.3984, val_loss: 0.0032, val_acc: 0.9030\n",
      "Epoch 9, train_loss: 0.5114, val_loss: 0.0030, val_acc: 0.9050\n",
      "Epoch 10, train_loss: 0.3531, val_loss: 0.0029, val_acc: 0.9030\n",
      "perceptron - alpha: 0, lr: 0.01, test acc: 0.8902, took: 45.33s\n",
      "Epoch 1, train_loss: 0.4750, val_loss: 0.0026, val_acc: 0.9150\n",
      "Epoch 2, train_loss: 0.3106, val_loss: 0.0024, val_acc: 0.9165\n",
      "Epoch 3, train_loss: 0.2863, val_loss: 0.0023, val_acc: 0.9195\n",
      "Epoch 4, train_loss: 0.2613, val_loss: 0.0022, val_acc: 0.9260\n",
      "Epoch 5, train_loss: 0.2982, val_loss: 0.0022, val_acc: 0.9280\n",
      "Epoch 6, train_loss: 0.4049, val_loss: 0.0021, val_acc: 0.9270\n",
      "perceptron - alpha: 0, lr: 0.05, test acc: 0.9085, took: 26.59s\n",
      "Epoch 1, train_loss: 0.3776, val_loss: 0.0021, val_acc: 0.9270\n",
      "Epoch 2, train_loss: 0.1021, val_loss: 0.0021, val_acc: 0.9310\n",
      "Epoch 3, train_loss: 0.2844, val_loss: 0.0021, val_acc: 0.9300\n",
      "Epoch 4, train_loss: 0.2665, val_loss: 0.0020, val_acc: 0.9325\n",
      "Epoch 5, train_loss: 0.1928, val_loss: 0.0020, val_acc: 0.9325\n",
      "perceptron - alpha: 0, lr: 0.1, test acc: 0.9156, took: 21.76s\n",
      "Epoch 1, train_loss: 0.1362, val_loss: 0.0021, val_acc: 0.9235\n",
      "Epoch 2, train_loss: 0.1723, val_loss: 0.0022, val_acc: 0.9175\n",
      "Epoch 3, train_loss: 0.3378, val_loss: 0.0021, val_acc: 0.9230\n",
      "Epoch 4, train_loss: 0.3034, val_loss: 0.0021, val_acc: 0.9255\n",
      "Epoch 5, train_loss: 0.1531, val_loss: 0.0021, val_acc: 0.9225\n",
      "perceptron - alpha: 0, lr: 0.5, test acc: 0.9124, took: 21.49s\n",
      "Epoch 1, train_loss: 0.9734, val_loss: 0.0064, val_acc: 0.8675\n",
      "Epoch 2, train_loss: 0.6469, val_loss: 0.0040, val_acc: 0.8940\n",
      "Epoch 3, train_loss: 0.4914, val_loss: 0.0032, val_acc: 0.9025\n",
      "Epoch 4, train_loss: 0.3714, val_loss: 0.0028, val_acc: 0.9155\n",
      "Epoch 5, train_loss: 0.4364, val_loss: 0.0026, val_acc: 0.9165\n",
      "Epoch 6, train_loss: 0.3112, val_loss: 0.0025, val_acc: 0.9210\n",
      "Epoch 7, train_loss: 0.3151, val_loss: 0.0024, val_acc: 0.9200\n",
      "MLP1 - alpha: 0, lr: 0.01, test acc: 0.9002, took: 34.99s\n",
      "Epoch 1, train_loss: 0.4375, val_loss: 0.0021, val_acc: 0.9295\n",
      "Epoch 2, train_loss: 0.2086, val_loss: 0.0019, val_acc: 0.9330\n",
      "Epoch 3, train_loss: 0.1964, val_loss: 0.0017, val_acc: 0.9430\n",
      "Epoch 4, train_loss: 0.2540, val_loss: 0.0016, val_acc: 0.9455\n",
      "Epoch 5, train_loss: 0.1646, val_loss: 0.0015, val_acc: 0.9505\n",
      "Epoch 6, train_loss: 0.2049, val_loss: 0.0014, val_acc: 0.9575\n",
      "Epoch 7, train_loss: 0.1796, val_loss: 0.0012, val_acc: 0.9595\n",
      "Epoch 8, train_loss: 0.1323, val_loss: 0.0011, val_acc: 0.9640\n",
      "Epoch 9, train_loss: 0.1364, val_loss: 0.0011, val_acc: 0.9670\n",
      "Epoch 10, train_loss: 0.1594, val_loss: 0.0010, val_acc: 0.9665\n",
      "MLP1 - alpha: 0, lr: 0.05, test acc: 0.9537, took: 49.64s\n",
      "Epoch 1, train_loss: 0.1844, val_loss: 0.0009, val_acc: 0.9670\n",
      "Epoch 2, train_loss: 0.1672, val_loss: 0.0009, val_acc: 0.9730\n",
      "Epoch 3, train_loss: 0.0464, val_loss: 0.0008, val_acc: 0.9700\n",
      "Epoch 4, train_loss: 0.1001, val_loss: 0.0008, val_acc: 0.9740\n",
      "Epoch 5, train_loss: 0.1188, val_loss: 0.0007, val_acc: 0.9760\n",
      "Epoch 6, train_loss: 0.2059, val_loss: 0.0008, val_acc: 0.9720\n",
      "MLP1 - alpha: 0, lr: 0.1, test acc: 0.9659, took: 31.17s\n",
      "Epoch 1, train_loss: 0.0775, val_loss: 0.0007, val_acc: 0.9715\n",
      "Epoch 2, train_loss: 0.1158, val_loss: 0.0007, val_acc: 0.9755\n",
      "Epoch 3, train_loss: 0.0158, val_loss: 0.0006, val_acc: 0.9780\n",
      "Epoch 4, train_loss: 0.1003, val_loss: 0.0007, val_acc: 0.9745\n",
      "Epoch 5, train_loss: 0.1523, val_loss: 0.0007, val_acc: 0.9690\n",
      "MLP1 - alpha: 0, lr: 0.5, test acc: 0.9698, took: 27.62s\n",
      "Epoch 1, train_loss: 1.2408, val_loss: 0.0096, val_acc: 0.7985\n",
      "Epoch 2, train_loss: 0.8121, val_loss: 0.0048, val_acc: 0.8710\n",
      "Epoch 3, train_loss: 0.4969, val_loss: 0.0035, val_acc: 0.8980\n",
      "Epoch 4, train_loss: 0.3046, val_loss: 0.0030, val_acc: 0.9025\n",
      "Epoch 5, train_loss: 0.3621, val_loss: 0.0027, val_acc: 0.9115\n",
      "Epoch 6, train_loss: 0.4809, val_loss: 0.0025, val_acc: 0.9135\n",
      "Epoch 7, train_loss: 0.5291, val_loss: 0.0023, val_acc: 0.9200\n",
      "Epoch 8, train_loss: 0.3702, val_loss: 0.0022, val_acc: 0.9210\n",
      "Epoch 9, train_loss: 0.2551, val_loss: 0.0021, val_acc: 0.9260\n",
      "Epoch 10, train_loss: 0.3222, val_loss: 0.0021, val_acc: 0.9235\n",
      "MLP2 - alpha: 0, lr: 0.01, test acc: 0.9075, took: 60.58s\n",
      "Epoch 1, train_loss: 0.1772, val_loss: 0.0019, val_acc: 0.9345\n",
      "Epoch 2, train_loss: 0.2373, val_loss: 0.0017, val_acc: 0.9375\n",
      "Epoch 3, train_loss: 0.1303, val_loss: 0.0016, val_acc: 0.9450\n",
      "Epoch 4, train_loss: 0.1624, val_loss: 0.0014, val_acc: 0.9505\n",
      "Epoch 5, train_loss: 0.3111, val_loss: 0.0014, val_acc: 0.9525\n",
      "Epoch 6, train_loss: 0.1946, val_loss: 0.0013, val_acc: 0.9575\n",
      "Epoch 7, train_loss: 0.2730, val_loss: 0.0011, val_acc: 0.9595\n",
      "Epoch 8, train_loss: 0.1261, val_loss: 0.0010, val_acc: 0.9640\n",
      "Epoch 9, train_loss: 0.3050, val_loss: 0.0009, val_acc: 0.9700\n",
      "Epoch 10, train_loss: 0.1484, val_loss: 0.0009, val_acc: 0.9680\n",
      "MLP2 - alpha: 0, lr: 0.05, test acc: 0.9569, took: 58.52s\n",
      "Epoch 1, train_loss: 0.0874, val_loss: 0.0009, val_acc: 0.9670\n",
      "Epoch 2, train_loss: 0.0763, val_loss: 0.0008, val_acc: 0.9710\n",
      "Epoch 3, train_loss: 0.1569, val_loss: 0.0008, val_acc: 0.9665\n",
      "Epoch 4, train_loss: 0.1799, val_loss: 0.0007, val_acc: 0.9720\n",
      "Epoch 5, train_loss: 0.0640, val_loss: 0.0006, val_acc: 0.9770\n",
      "Epoch 6, train_loss: 0.0877, val_loss: 0.0006, val_acc: 0.9770\n",
      "MLP2 - alpha: 0, lr: 0.1, test acc: 0.9712, took: 34.93s\n",
      "Epoch 1, train_loss: 0.0665, val_loss: 0.0010, val_acc: 0.9600\n",
      "Epoch 2, train_loss: 0.0390, val_loss: 0.0007, val_acc: 0.9775\n",
      "Epoch 3, train_loss: 0.1639, val_loss: 0.0011, val_acc: 0.9565\n",
      "Epoch 4, train_loss: 0.0605, val_loss: 0.0007, val_acc: 0.9740\n",
      "Epoch 5, train_loss: 0.1081, val_loss: 0.0005, val_acc: 0.9810\n",
      "Epoch 6, train_loss: 0.0202, val_loss: 0.0005, val_acc: 0.9835\n",
      "Epoch 7, train_loss: 0.0731, val_loss: 0.0006, val_acc: 0.9815\n",
      "MLP2 - alpha: 0, lr: 0.5, test acc: 0.9728, took: 42.76s\n",
      "Epoch 1, train_loss: 1.6219, val_loss: 0.0129, val_acc: 0.6230\n",
      "Epoch 2, train_loss: 0.7911, val_loss: 0.0063, val_acc: 0.8030\n",
      "Epoch 3, train_loss: 0.7287, val_loss: 0.0044, val_acc: 0.8505\n",
      "Epoch 4, train_loss: 0.5083, val_loss: 0.0035, val_acc: 0.8780\n",
      "Epoch 5, train_loss: 0.6331, val_loss: 0.0030, val_acc: 0.8950\n",
      "Epoch 6, train_loss: 0.4461, val_loss: 0.0027, val_acc: 0.9030\n",
      "Epoch 7, train_loss: 0.2504, val_loss: 0.0025, val_acc: 0.9115\n",
      "Epoch 8, train_loss: 0.2773, val_loss: 0.0023, val_acc: 0.9175\n",
      "Epoch 9, train_loss: 0.3032, val_loss: 0.0022, val_acc: 0.9225\n",
      "Epoch 10, train_loss: 0.3845, val_loss: 0.0022, val_acc: 0.9215\n",
      "MLP4 - alpha: 0, lr: 0.01, test acc: 0.9016, took: 69.27s\n",
      "Epoch 1, train_loss: 0.3110, val_loss: 0.0019, val_acc: 0.9270\n",
      "Epoch 2, train_loss: 0.2616, val_loss: 0.0018, val_acc: 0.9365\n",
      "Epoch 3, train_loss: 0.0927, val_loss: 0.0014, val_acc: 0.9500\n",
      "Epoch 4, train_loss: 0.1376, val_loss: 0.0013, val_acc: 0.9570\n",
      "Epoch 5, train_loss: 0.1634, val_loss: 0.0012, val_acc: 0.9565\n",
      "MLP4 - alpha: 0, lr: 0.05, test acc: 0.9429, took: 36.16s\n",
      "Epoch 1, train_loss: 0.1349, val_loss: 0.0011, val_acc: 0.9615\n",
      "Epoch 2, train_loss: 0.2647, val_loss: 0.0010, val_acc: 0.9620\n",
      "Epoch 3, train_loss: 0.0778, val_loss: 0.0008, val_acc: 0.9680\n",
      "Epoch 4, train_loss: 0.0698, val_loss: 0.0007, val_acc: 0.9725\n",
      "Epoch 5, train_loss: 0.1045, val_loss: 0.0007, val_acc: 0.9740\n",
      "Epoch 6, train_loss: 0.0791, val_loss: 0.0006, val_acc: 0.9765\n",
      "Epoch 7, train_loss: 0.0628, val_loss: 0.0007, val_acc: 0.9770\n",
      "Epoch 8, train_loss: 0.1639, val_loss: 0.0006, val_acc: 0.9790\n",
      "Epoch 9, train_loss: 0.0968, val_loss: 0.0005, val_acc: 0.9770\n",
      "MLP4 - alpha: 0, lr: 0.1, test acc: 0.9728, took: 60.93s\n",
      "Epoch 1, train_loss: 0.2412, val_loss: 0.0009, val_acc: 0.9635\n",
      "Epoch 2, train_loss: 0.1015, val_loss: 0.0007, val_acc: 0.9760\n",
      "Epoch 3, train_loss: 0.0683, val_loss: 0.0006, val_acc: 0.9780\n",
      "Epoch 4, train_loss: 0.0267, val_loss: 0.0006, val_acc: 0.9780\n",
      "Epoch 5, train_loss: 0.1414, val_loss: 0.0011, val_acc: 0.9615\n",
      "MLP4 - alpha: 0, lr: 0.5, test acc: 0.9514, took: 33.72s\n",
      "25 1560000\n",
      "Epoch 1, train_loss: 2.4577, val_loss: 0.0162, val_acc: 0.3035\n",
      "Epoch 2, train_loss: 2.4464, val_loss: 0.0166, val_acc: 0.3480\n",
      "Epoch 3, train_loss: 2.3635, val_loss: 0.0166, val_acc: 0.4190\n",
      "Epoch 4, train_loss: 2.3351, val_loss: 0.0167, val_acc: 0.4845\n",
      "Epoch 5, train_loss: 2.3077, val_loss: 0.0168, val_acc: 0.5090\n",
      "Epoch 6, train_loss: 2.3050, val_loss: 0.0168, val_acc: 0.5590\n",
      "Epoch 7, train_loss: 2.3234, val_loss: 0.0168, val_acc: 0.5675\n",
      "Epoch 8, train_loss: 2.3191, val_loss: 0.0168, val_acc: 0.6425\n",
      "Epoch 9, train_loss: 2.3232, val_loss: 0.0169, val_acc: 0.6025\n",
      "perceptron - alpha: 25, lr: 0.01, test acc: 0.5740, took: 1069.77s\n",
      "Epoch 1, train_loss: 2.3065, val_loss: 0.0169, val_acc: 0.3340\n",
      "Epoch 2, train_loss: 2.3039, val_loss: 0.0171, val_acc: 0.3975\n",
      "Epoch 3, train_loss: 2.3060, val_loss: 0.0168, val_acc: 0.5840\n",
      "Epoch 4, train_loss: 2.2990, val_loss: 0.0170, val_acc: 0.4865\n",
      "Epoch 5, train_loss: 2.3223, val_loss: 0.0170, val_acc: 0.5005\n",
      "perceptron - alpha: 25, lr: 0.05, test acc: 0.4885, took: 591.38s\n",
      "Epoch 1, train_loss: 2.2732, val_loss: 0.0169, val_acc: 0.3880\n",
      "Epoch 2, train_loss: 2.2650, val_loss: 0.0176, val_acc: 0.1285\n",
      "Epoch 3, train_loss: 2.3417, val_loss: 0.0170, val_acc: 0.4720\n",
      "Epoch 4, train_loss: 2.3073, val_loss: 0.0170, val_acc: 0.4255\n",
      "Epoch 5, train_loss: 2.3060, val_loss: 0.0170, val_acc: 0.3605\n",
      "perceptron - alpha: 25, lr: 0.1, test acc: 0.3406, took: 599.53s\n",
      "Epoch 1, train_loss: 2.3301, val_loss: 0.0179, val_acc: 0.1655\n",
      "Epoch 2, train_loss: 2.4195, val_loss: 0.0179, val_acc: 0.1350\n",
      "Epoch 3, train_loss: 2.5702, val_loss: 0.0187, val_acc: 0.1640\n",
      "Epoch 4, train_loss: 2.5086, val_loss: 0.0190, val_acc: 0.1470\n",
      "Epoch 5, train_loss: 2.3388, val_loss: 0.0180, val_acc: 0.1210\n",
      "perceptron - alpha: 25, lr: 0.5, test acc: 0.1320, took: 586.22s\n",
      "Epoch 1, train_loss: 2.3077, val_loss: 0.0179, val_acc: 0.4115\n",
      "Epoch 2, train_loss: 2.2922, val_loss: 0.0177, val_acc: 0.5410\n",
      "Epoch 3, train_loss: 2.3122, val_loss: 0.0176, val_acc: 0.6195\n",
      "Epoch 4, train_loss: 2.3002, val_loss: 0.0175, val_acc: 0.7280\n",
      "Epoch 5, train_loss: 2.2949, val_loss: 0.0173, val_acc: 0.7645\n",
      "Epoch 6, train_loss: 2.3097, val_loss: 0.0172, val_acc: 0.7980\n",
      "Epoch 7, train_loss: 2.3023, val_loss: 0.0171, val_acc: 0.8205\n",
      "Epoch 8, train_loss: 2.3174, val_loss: 0.0171, val_acc: 0.8330\n",
      "Epoch 9, train_loss: 2.3149, val_loss: 0.0170, val_acc: 0.8405\n",
      "Epoch 10, train_loss: 2.3210, val_loss: 0.0170, val_acc: 0.8435\n",
      "Epoch 11, train_loss: 2.3097, val_loss: 0.0170, val_acc: 0.8650\n",
      "Epoch 12, train_loss: 2.3136, val_loss: 0.0169, val_acc: 0.8625\n",
      "MLP1 - alpha: 25, lr: 0.01, test acc: 0.8395, took: 1641.75s\n",
      "Epoch 1, train_loss: 2.3029, val_loss: 0.0169, val_acc: 0.8460\n",
      "Epoch 2, train_loss: 2.2893, val_loss: 0.0167, val_acc: 0.8800\n",
      "Epoch 3, train_loss: 2.2940, val_loss: 0.0166, val_acc: 0.8700\n",
      "Epoch 4, train_loss: 2.2862, val_loss: 0.0167, val_acc: 0.8590\n",
      "Epoch 5, train_loss: 2.2889, val_loss: 0.0168, val_acc: 0.8660\n",
      "MLP1 - alpha: 25, lr: 0.05, test acc: 0.8375, took: 681.70s\n",
      "Epoch 1, train_loss: 2.3010, val_loss: 0.0167, val_acc: 0.8790\n",
      "Epoch 2, train_loss: 2.2953, val_loss: 0.0166, val_acc: 0.8695\n",
      "Epoch 3, train_loss: 2.3121, val_loss: 0.0167, val_acc: 0.8375\n",
      "Epoch 4, train_loss: 2.3123, val_loss: 0.0166, val_acc: 0.8605\n",
      "Epoch 5, train_loss: 2.3006, val_loss: 0.0166, val_acc: 0.8950\n",
      "Epoch 6, train_loss: 2.3011, val_loss: 0.0170, val_acc: 0.8505\n",
      "MLP1 - alpha: 25, lr: 0.1, test acc: 0.8363, took: 819.49s\n",
      "Epoch 1, train_loss: 2.3035, val_loss: 0.0179, val_acc: 0.3910\n",
      "Epoch 2, train_loss: 2.2905, val_loss: 0.0178, val_acc: 0.4235\n",
      "Epoch 3, train_loss: 2.3058, val_loss: 0.0180, val_acc: 0.2805\n",
      "Epoch 4, train_loss: 2.2882, val_loss: 0.0182, val_acc: 0.2550\n",
      "Epoch 5, train_loss: 2.2954, val_loss: 0.0177, val_acc: 0.3675\n",
      "MLP1 - alpha: 25, lr: 0.5, test acc: 0.3489, took: 681.51s\n",
      "Epoch 1, train_loss: 2.2984, val_loss: 0.0179, val_acc: 0.4895\n",
      "Epoch 2, train_loss: 2.2983, val_loss: 0.0177, val_acc: 0.6305\n",
      "Epoch 3, train_loss: 2.3079, val_loss: 0.0175, val_acc: 0.6715\n",
      "Epoch 4, train_loss: 2.2926, val_loss: 0.0174, val_acc: 0.7155\n",
      "Epoch 5, train_loss: 2.3183, val_loss: 0.0173, val_acc: 0.7465\n",
      "Epoch 6, train_loss: 2.3061, val_loss: 0.0173, val_acc: 0.7750\n",
      "Epoch 7, train_loss: 2.3095, val_loss: 0.0172, val_acc: 0.7745\n",
      "MLP2 - alpha: 25, lr: 0.01, test acc: 0.7552, took: 1111.40s\n",
      "Epoch 1, train_loss: 2.3110, val_loss: 0.0171, val_acc: 0.8195\n",
      "Epoch 2, train_loss: 2.3039, val_loss: 0.0170, val_acc: 0.8315\n",
      "Epoch 3, train_loss: 2.2782, val_loss: 0.0169, val_acc: 0.8880\n",
      "Epoch 4, train_loss: 2.2954, val_loss: 0.0167, val_acc: 0.8745\n",
      "Epoch 5, train_loss: 2.3130, val_loss: 0.0167, val_acc: 0.9010\n",
      "Epoch 6, train_loss: 2.3073, val_loss: 0.0167, val_acc: 0.8845\n",
      "MLP2 - alpha: 25, lr: 0.05, test acc: 0.8714, took: 920.70s\n",
      "Epoch 1, train_loss: 2.2902, val_loss: 0.0168, val_acc: 0.8885\n",
      "Epoch 2, train_loss: 2.3082, val_loss: 0.0165, val_acc: 0.8795\n",
      "Epoch 3, train_loss: 2.3052, val_loss: 0.0166, val_acc: 0.9015\n",
      "Epoch 4, train_loss: 2.2927, val_loss: 0.0165, val_acc: 0.9075\n",
      "Epoch 5, train_loss: 2.3088, val_loss: 0.0167, val_acc: 0.8845\n",
      "MLP2 - alpha: 25, lr: 0.1, test acc: 0.8889, took: 788.12s\n",
      "Epoch 1, train_loss: 2.2965, val_loss: 0.0172, val_acc: 0.7240\n",
      "Epoch 2, train_loss: 2.3158, val_loss: 0.0169, val_acc: 0.8105\n",
      "Epoch 3, train_loss: 2.2931, val_loss: 0.0173, val_acc: 0.6585\n",
      "Epoch 4, train_loss: 2.3125, val_loss: 0.0173, val_acc: 0.7370\n",
      "Epoch 5, train_loss: 2.3017, val_loss: 0.0168, val_acc: 0.8235\n",
      "Epoch 6, train_loss: 2.3059, val_loss: 0.0172, val_acc: 0.6775\n",
      "MLP2 - alpha: 25, lr: 0.5, test acc: 0.6549, took: 943.19s\n",
      "Epoch 1, train_loss: 2.3081, val_loss: 0.0178, val_acc: 0.5260\n",
      "Epoch 2, train_loss: 2.3044, val_loss: 0.0175, val_acc: 0.6725\n",
      "Epoch 3, train_loss: 2.3124, val_loss: 0.0173, val_acc: 0.7845\n",
      "Epoch 4, train_loss: 2.3033, val_loss: 0.0171, val_acc: 0.8400\n",
      "Epoch 5, train_loss: 2.3047, val_loss: 0.0170, val_acc: 0.8685\n",
      "Epoch 6, train_loss: 2.2852, val_loss: 0.0168, val_acc: 0.8900\n",
      "Epoch 7, train_loss: 2.3104, val_loss: 0.0169, val_acc: 0.8930\n",
      "Epoch 8, train_loss: 2.2959, val_loss: 0.0167, val_acc: 0.9080\n",
      "Epoch 9, train_loss: 2.3032, val_loss: 0.0167, val_acc: 0.9240\n",
      "Epoch 10, train_loss: 2.2986, val_loss: 0.0167, val_acc: 0.9245\n",
      "Epoch 11, train_loss: 2.2810, val_loss: 0.0166, val_acc: 0.9280\n",
      "Epoch 12, train_loss: 2.2843, val_loss: 0.0166, val_acc: 0.9315\n",
      "Epoch 13, train_loss: 2.2850, val_loss: 0.0166, val_acc: 0.9390\n",
      "Epoch 14, train_loss: 2.3033, val_loss: 0.0166, val_acc: 0.9415\n",
      "Epoch 15, train_loss: 2.2939, val_loss: 0.0165, val_acc: 0.9400\n",
      "MLP4 - alpha: 25, lr: 0.01, test acc: 0.9199, took: 2823.05s\n",
      "Epoch 1, train_loss: 2.2959, val_loss: 0.0165, val_acc: 0.9375\n",
      "Epoch 2, train_loss: 2.2842, val_loss: 0.0164, val_acc: 0.9435\n",
      "Epoch 3, train_loss: 2.2956, val_loss: 0.0165, val_acc: 0.9430\n",
      "Epoch 4, train_loss: 2.3047, val_loss: 0.0165, val_acc: 0.9435\n",
      "Epoch 5, train_loss: 2.3024, val_loss: 0.0164, val_acc: 0.9390\n",
      "MLP4 - alpha: 25, lr: 0.05, test acc: 0.9275, took: 2696.42s\n",
      "Epoch 1, train_loss: 2.2881, val_loss: 0.0166, val_acc: 0.9275\n",
      "Epoch 2, train_loss: 2.2767, val_loss: 0.0163, val_acc: 0.9415\n",
      "Epoch 3, train_loss: 2.3071, val_loss: 0.0164, val_acc: 0.9345\n",
      "Epoch 4, train_loss: 2.2900, val_loss: 0.0165, val_acc: 0.9245\n",
      "Epoch 5, train_loss: 2.2970, val_loss: 0.0164, val_acc: 0.9420\n",
      "Epoch 6, train_loss: 2.2955, val_loss: 0.0164, val_acc: 0.9400\n",
      "MLP4 - alpha: 25, lr: 0.1, test acc: 0.9286, took: 5318.97s\n",
      "Epoch 1, train_loss: 2.2964, val_loss: 0.0167, val_acc: 0.8395\n",
      "Epoch 2, train_loss: 2.2827, val_loss: 0.0169, val_acc: 0.6935\n",
      "Epoch 3, train_loss: 2.2917, val_loss: 0.0167, val_acc: 0.8360\n",
      "Epoch 4, train_loss: 2.2813, val_loss: 0.0169, val_acc: 0.8560\n",
      "Epoch 5, train_loss: 2.3057, val_loss: 0.0170, val_acc: 0.8365\n",
      "MLP4 - alpha: 25, lr: 0.5, test acc: 0.8306, took: 1897.71s\n",
      "50 3060000\n",
      "Epoch 1, train_loss: 2.2935, val_loss: 0.0176, val_acc: 0.5275\n",
      "Epoch 2, train_loss: 2.3140, val_loss: 0.0176, val_acc: 0.5930\n",
      "Epoch 3, train_loss: 2.2826, val_loss: 0.0176, val_acc: 0.6570\n",
      "Epoch 4, train_loss: 2.2898, val_loss: 0.0177, val_acc: 0.5320\n",
      "Epoch 5, train_loss: 2.3092, val_loss: 0.0177, val_acc: 0.4895\n",
      "perceptron - alpha: 50, lr: 0.01, test acc: 0.4755, took: 1171.92s\n",
      "Epoch 1, train_loss: 2.3184, val_loss: 0.0175, val_acc: 0.3215\n",
      "Epoch 2, train_loss: 2.3203, val_loss: 0.0177, val_acc: 0.1570\n",
      "Epoch 3, train_loss: 2.2966, val_loss: 0.0178, val_acc: 0.2590\n",
      "Epoch 4, train_loss: 2.3140, val_loss: 0.0178, val_acc: 0.1980\n",
      "Epoch 5, train_loss: 2.3046, val_loss: 0.0178, val_acc: 0.2745\n",
      "Epoch 6, train_loss: 2.3184, val_loss: 0.0177, val_acc: 0.2560\n",
      "perceptron - alpha: 50, lr: 0.05, test acc: 0.2844, took: 59533.61s\n",
      "Epoch 1, train_loss: 2.3408, val_loss: 0.0181, val_acc: 0.0990\n",
      "Epoch 2, train_loss: 2.2724, val_loss: 0.0180, val_acc: 0.1180\n",
      "Epoch 3, train_loss: 2.2891, val_loss: 0.0182, val_acc: 0.0890\n"
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "\n",
    "    noisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = alpha)\n",
    "    train_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    print(alpha, len(noisy_train_dataset))\n",
    "\n",
    "    for (name, model), lr in itertools.product(models.items(), learning_rates):\n",
    "\n",
    "        start = time.time()\n",
    "        model, log = train(model, train_loader, val_loader, lr = lr, verbose = True)\n",
    "        test_loss, test_acc = eval_model(model, test_loader)\n",
    "        log['test_loss'] = test_loss\n",
    "        log['test_acc'] = test_acc\n",
    "\n",
    "        print(f'{name} - alpha: {alpha}, lr: {lr}, test acc: {test_acc:.4f}, took: {time.time() - start:.2f}s')\n",
    "        torch.save(log, f'logs/{name}_{alpha}_{lr}.pt')\n",
    "        torch.save(model, f'models/{name}_{alpha}_{lr}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9145, 0.724, 0.780625, 0.735]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get name of files in logs/perceptron*\n",
    "best_lr_file = lambda m, a: max([f for f in pathlib.Path('logs').glob(f'{m}_{a}*')], key = lambda f: torch.load(f)['test_acc'])\n",
    "best_lr_file('perceptron', 10)\n",
    "test_accs = [torch.load(best_lr_file('perceptron', a))['test_acc'] for a in range(0, 40, 10)]\n",
    "test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- does early stopping help?\n",
    "- does order matter: intuitively if we put all the true labelled exampes at the start (of the epoch) it should perform worse and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
