{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Learning is Robust to Massive Label Noise](https://arxiv.org/pdf/1705.10694)\n",
    "\n",
    "\n",
    "The paper shows that neural networks can generalize when large numbers of (non-adversarially) incorrectly labeled examples are added to datasets (MNIST, CIFAR, and ImageNet).\n",
    "\n",
    "We'll focus on uniform label noise (Experiment 1) using MNIST first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, itertools\n",
    "\n",
    "os.makedirs('logs', exist_ok = True)\n",
    "os.makedirs('models', exist_ok = True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = torch.device(\n",
    "#     'cuda' if torch.cuda.is_available() else\n",
    "#     ('mps' if torch.backends.mps.is_available() else\n",
    "#     'cpu')\n",
    "# )\n",
    "device = 'cpu' # faster for the small models we are using\n",
    "\n",
    "def eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n",
    "    # Returns loss and accuracy of the model on the test set\n",
    "    model.eval()\n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, pred = torch.max(model(images), 1)\n",
    "            correct += (pred == labels).float().sum().item()\n",
    "            loss += criterion(model(images), labels).item()\n",
    "    return loss / len(test.dataset), correct / len(test.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLabelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Adds alpha uniformly noisy labels for every example in the original dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, alpha = 1):\n",
    "        self.dataset = dataset\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def is_noisy(self, idx):\n",
    "        return idx % (self.alpha + 1) != 0\n",
    "\n",
    "    def __len__(self):\n",
    "        n = len(self.dataset)\n",
    "        return n + (self.alpha * n)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx // (self.alpha + 1)]\n",
    "        if self.is_noisy(idx):\n",
    "            y = np.random.choice(len(self.dataset.classes))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "original_train_dataset = datasets.MNIST('data', download = True, train = True, transform = transform)\n",
    "\n",
    "test_dataset = datasets.MNIST('data', download = True, train = False, transform = transform)\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (0.2, 0.8), generator = torch.Generator().manual_seed(seed))\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, lr = 0.01, patience = 3, max_epochs = 100, verbose = False):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr = lr)\n",
    "\n",
    "    log = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_acc = eval_model(model, val_loader)\n",
    "        log['train_loss'].append(loss.item())\n",
    "        log['val_loss'].append(val_loss)\n",
    "        log['val_acc'].append(val_acc)\n",
    "\n",
    "        if verbose: print(', '.join([f'Epoch {epoch + 1}'] + [f'{k}: {v[-1]:.4f}' for k, v in log.items()]))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        # Early stopping: stop if val loss has not decreased in the last patience epochs\n",
    "        if epoch > patience and val_loss >= min(log['val_loss'][-patience-1:-1]): break \n",
    "    \n",
    "    if best_model: model.load_state_dict(best_model)\n",
    "    return model, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
    "alphas = [0, 10, 100] #list(range(0, 110, 10))\n",
    "\n",
    "lin_relu = lambda n_in, n_out: nn.Sequential(nn.Linear(n_in, n_out), nn.ReLU())\n",
    "models = {\n",
    "    'perceptron':nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)),\n",
    "    'MLP1':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), nn.Linear(256, 10)),\n",
    "    'MLP2':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), nn.Linear(128, 10)),\n",
    "    'MLP4':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), lin_relu(128, 64), nn.Linear(64, 10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceptron - alpha: 0, lr: 0.01, test acc: 0.9051\n",
      "perceptron - alpha: 0, lr: 0.05, test acc: 0.9155\n",
      "perceptron - alpha: 0, lr: 0.1, test acc: 0.9143\n",
      "perceptron - alpha: 0, lr: 0.5, test acc: 0.9163\n",
      "perceptron - alpha: 10, lr: 0.01, test acc: 0.5196\n"
     ]
    }
   ],
   "source": [
    "for (name, model), alpha, lr in itertools.product(models.items(), alphas, learning_rates):\n",
    "\n",
    "    train_dataset = NoisyLabelDataset(original_train_dataset, alpha = alpha)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    model, log = train(model, train_loader, val_loader, lr = lr)\n",
    "    test_loss, test_acc = eval_model(model, test_loader)\n",
    "    log['test_loss'] = test_loss\n",
    "    log['test_acc'] = test_acc\n",
    "    print(f'{name} - alpha: {alpha}, lr: {lr}, test acc: {test_acc:.4f}')\n",
    "\n",
    "    torch.save(log, f'logs/{name}_{alpha}_{lr}.pt')\n",
    "    torch.save(model, f'models/{name}_{alpha}_{lr}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9145, 0.724, 0.780625, 0.735]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get name of files in logs/perceptron*\n",
    "best_lr_file = lambda m, a: max([f for f in pathlib.Path('logs').glob(f'{m}_{a}*')], key = lambda f: torch.load(f)['test_acc'])\n",
    "best_lr_file('perceptron', 10)\n",
    "test_accs = [torch.load(best_lr_file('perceptron', a))['test_acc'] for a in range(0, 40, 10)]\n",
    "test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- does order matter: intuitively if we put all the true labelled exampes at the start (of the epoch) it should perform worse and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
