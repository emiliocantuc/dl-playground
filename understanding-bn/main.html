<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="E. Cantu">

<title>Understanding Batch Normalization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="main_files/libs/clipboard/clipboard.min.js"></script>
<script src="main_files/libs/quarto-html/quarto.js"></script>
<script src="main_files/libs/quarto-html/popper.min.js"></script>
<script src="main_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="main_files/libs/quarto-html/anchor.min.js"></script>
<link href="main_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="main_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="main_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="main_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="main_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#convolutional-bn-layer" id="toc-convolutional-bn-layer" class="nav-link active" data-scroll-target="#convolutional-bn-layer">Convolutional BN Layer</a></li>
  <li><a href="#experimental-setup" id="toc-experimental-setup" class="nav-link" data-scroll-target="#experimental-setup">Experimental setup</a></li>
  <li><a href="#what-i-learned" id="toc-what-i-learned" class="nav-link" data-scroll-target="#what-i-learned">What I learned</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding Batch Normalization</h1>
<p class="subtitle lead">Attempted (partial) paper reproduction</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>E. Cantu </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><a href="https://arxiv.org/pdf/1806.02375">Paper</a> | <a href="main.ipynb">This notebook</a></p>
<p>The paper investigates the cause of batch norm’s benefits experimentally. The authors show that its main benefit is allowing for larger learning rates during training. In particular:</p>
<blockquote class="blockquote">
<p>“We show that the activations and gradients in deep neural networks without BN tend to be heavy-tailed. In particular, during an early on-set of divergence, a small subset of activations (typically in deep layer) “explode”. The typical practice to avoid such divergence is to set the learning rate to be sufficiently small such that no steep gradient direction can lead to divergence. However, small learning rates yield little progress along flat directions of the optimization landscape and may be more prone to convergence to sharp local minima with possibly worse generalization performance.”</p>
</blockquote>
<p>The experiments are primarily done with a ResNet 110 trained on CIFAR-10. We will attempt to reproduce figures 1-6.</p>
<section id="convolutional-bn-layer" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-bn-layer">Convolutional BN Layer</h3>
<p>As a reminder, the input <span class="math inline">\(I\)</span> and output <span class="math inline">\(O\)</span> tensors to a batch norm layer are 4 dimentional. The dimentions <span class="math inline">\((b, c, x, y)\)</span> correspond to the batch example, channel and spatial <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> dimentions respectively. Batch norm (BN) applies a channel-wise normalization:</p>
<p><span class="math display">\[
O_{b, c, x, y} \leftarrow \gamma_c \frac{I_{b, c, x, y} - \hat \mu_c}{\sqrt{\hat \sigma_c^2 + \epsilon}} + \beta_c
\]</span></p>
<p>Where <span class="math inline">\(\hat \mu_c\)</span> and <span class="math inline">\(\hat \sigma_c^2\)</span> are estimates channel <span class="math inline">\(c\)</span>’s mean and standard deviation computed on the minibatch <span class="math inline">\(\mathcal B\)</span>:</p>
<p><span class="math display">\[
\hat \mu_c = \frac{1}{|\mathcal B|}\sum_{b, x, y} I_{b, c, x, y}
\]</span></p>
<p><span class="math display">\[
\hat \sigma_c^2 = \frac{1}{\mathcal |B|} \sum_{b, x, y} (I_{b, c, x, y} - \hat \mu_c) ^ 2
\]</span></p>
<p>To make sure the layer does not loose expressive power we introduce learned parameters <span class="math inline">\(\gamma_c\)</span> and <span class="math inline">\(\beta_c\)</span>. <span class="math inline">\(\epsilon\)</span> is a small constant added for numerical stability. In pytorch we can simply use the <code>BatchNorm2d</code> layer.</p>
</section>
<section id="experimental-setup" class="level3">
<h3 class="anchored" data-anchor-id="experimental-setup">Experimental setup</h3>
<p>Let’s setup our data loaders, model and training loop as described in Appendix B of the paper.</p>
<div id="cell-6" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Imports and model evaluation function</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms, models</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, itertools, time</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">'logs'</span>, exist_ok <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">'models'</span>, exist_ok <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'mps'</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cpu'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model, test, criterion <span class="op">=</span> nn.CrossEntropyLoss()):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    correct, loss <span class="op">=</span> <span class="dv">0</span>, <span class="fl">0.0</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> test:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            _, pred <span class="op">=</span> torch.<span class="bu">max</span>(model(images), <span class="dv">1</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (pred <span class="op">==</span> labels).<span class="bu">float</span>().<span class="bu">sum</span>().item()</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> criterion(model(images), labels).item()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss <span class="op">/</span> <span class="bu">len</span>(test.dataset), correct <span class="op">/</span> <span class="bu">len</span>(test.dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The paper trains ResNet-101s (110 in the paper is a typo?) on CIFAR-10, with channel-wise normalization, random horizontal flipping, and 32-by-32 cropping with 4-pixel zero padding.</p>
<p>We first get the datasets and compute the channel-wise means and variances. Note: both the training and validation set have the same values.</p>
<div id="cell-8" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Datasets and channel-wise means and stds</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> datasets.CIFAR10(<span class="st">'./data'</span>, download <span class="op">=</span> <span class="va">True</span>, train <span class="op">=</span> <span class="va">True</span>, transform <span class="op">=</span> transforms.ToTensor())</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>val <span class="op">=</span> datasets.CIFAR10(<span class="st">'./data'</span>, download <span class="op">=</span> <span class="va">True</span>, train <span class="op">=</span> <span class="va">False</span>, transform <span class="op">=</span> transforms.ToTensor())</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> channel_means_stds(dataset):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> torch.stack([img <span class="cf">for</span> img, _ <span class="kw">in</span> train])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> imgs.mean(dim <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>]), imgs.std(dim <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>means, stds <span class="op">=</span> channel_means_stds(train)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Training channel-wise</span><span class="ch">\n\t</span><span class="ss">means: </span><span class="sc">{</span>means<span class="sc">}</span><span class="ch">\n\t</span><span class="ss">stds: </span><span class="sc">{</span>stds<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>means, stds <span class="op">=</span> channel_means_stds(val)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Validation channel-wise</span><span class="ch">\n\t</span><span class="ss">means: </span><span class="sc">{</span>means<span class="sc">}</span><span class="ch">\n\t</span><span class="ss">stds: </span><span class="sc">{</span>stds<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We now define the transforms with data aumentation and data loaders with batch size <span class="math inline">\(128\)</span>.</p>
<div id="cell-10" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Data transforms and data loaders</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>train_transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    transforms.RandomCrop(<span class="dv">32</span>, padding <span class="op">=</span> <span class="dv">4</span>),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(means, stds),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># We do not perform data augmentation on the validation set</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>val_transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(means, stds),</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>train.transform <span class="op">=</span> train_transform</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>val.transform <span class="op">=</span> val_transform</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train, batch_size <span class="op">=</span> <span class="dv">128</span>, shuffle <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val, batch_size <span class="op">=</span> <span class="dv">128</span>, shuffle <span class="op">=</span> <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We’ll use <code>torchvision</code>’s implementation of ResNet-101, Xavier initialization, SGD with momentum <span class="math inline">\(0.9\)</span> and weight decay <span class="math inline">\(5\times 10^{-4}\)</span>, and cross entropy loss. We implement the learning rate schedule and training details mentioned:</p>
<blockquote class="blockquote">
<p>“Initially, all models are trained for 165 epochs and as in [17] we divide the learning rate by 10 after epoch 50% and 75%, at which point learning has typically plateued. If learning doesn’t plateu for some number of epochs, we roughly double the number of epochs until it does”.</p>
</blockquote>
<div id="cell-12" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Init, and train functions</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> xavier_init(m):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Conv2d) <span class="kw">or</span> <span class="bu">isinstance</span>(m, nn.Linear):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(m.weight)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, train, optimizer, criterion):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Trains the model for one epoch</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    train_loss, correct <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> train:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(images)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, labels)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        _, pred <span class="op">=</span> torch.<span class="bu">max</span>(output, <span class="dv">1</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> (pred <span class="op">==</span> labels).<span class="bu">float</span>().<span class="bu">sum</span>().item()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss <span class="op">/</span> <span class="bu">len</span>(train.dataset), correct <span class="op">/</span> <span class="bu">len</span>(train.dataset)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, train, val, init_lr, plateau_patience <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr <span class="op">=</span> init_lr, momentum <span class="op">=</span> <span class="fl">0.9</span>, weight_decay <span class="op">=</span> <span class="fl">5e-4</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    init_epochs <span class="op">=</span> <span class="dv">165</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    lr_schedule <span class="op">=</span> {</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span>(init_epochs <span class="op">*</span> <span class="fl">0.5</span>): init_lr <span class="op">//</span> <span class="dv">10</span>,</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span>(init_epochs <span class="op">*</span> <span class="fl">0.75</span>): init_lr <span class="op">//</span> <span class="dv">100</span>,</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    plateau_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    best_accuracy <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    train_losses, train_accs, val_losses, val_accs <span class="op">=</span> [], [], [], []</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> epoch <span class="op">&lt;</span> init_epochs <span class="kw">and</span> plateau_count <span class="op">&lt;</span> plateau_patience:</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the learning rate</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="kw">in</span> lr_schedule:</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param_group <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                param_group[<span class="st">'lr'</span>] <span class="op">=</span> lr_schedule[epoch]</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train the model for an epoch</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        loss, acc <span class="op">=</span> train_epoch(model, train, optimizer, criterion)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        train_losses.append(loss)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        train_accs.append(acc)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate the model on the validation set</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        val_loss, val_acc <span class="op">=</span> eval_model(model, val, criterion)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        val_losses.append(val_loss)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        val_accs.append(val_acc)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for a plateau</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> val_acc <span class="op">&gt;</span> best_accuracy:</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>            best_accuracy <span class="op">=</span> val_acc</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>            plateau_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>            plateau_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        epoch <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "If learning doesn’t plateu for some number of epochs,</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we roughly double the number of epochs until it does."</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">==</span> init_epochs <span class="kw">and</span> plateau_count <span class="op">&lt;</span> plateau_patience:</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>            init_epochs <span class="op">*=</span> <span class="dv">2</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>init_epochs<span class="sc">}</span><span class="ss"> | '</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'Training loss: </span><span class="sc">{</span>train_losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss"> | '</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'Validation loss: </span><span class="sc">{</span>val_losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss"> | '</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'Validation accuracy: </span><span class="sc">{</span>val_accs[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses, val_losses, val_accs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And we define a function to disable batch norm layers in a model by replacing them for identity layers:</p>
<div id="cell-14" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> disable_bn(model):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, module <span class="kw">in</span> model.named_children():</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.BatchNorm2d):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>            <span class="bu">setattr</span>(model, name, nn.Identity())</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            disable_bn(module)  <span class="co"># Recursively replace in child modules</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet101()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.requires_grad <span class="kw">and</span> <span class="bu">len</span>(param.size()) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(name, param.shape, n)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>conv1.weight torch.Size([64, 3, 7, 7]) 1
layer1.0.conv1.weight torch.Size([64, 64, 1, 1]) 2
layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) 3
layer1.0.conv3.weight torch.Size([256, 64, 1, 1]) 4
layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1]) 5
layer1.1.conv1.weight torch.Size([64, 256, 1, 1]) 6
layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) 7
layer1.1.conv3.weight torch.Size([256, 64, 1, 1]) 8
layer1.2.conv1.weight torch.Size([64, 256, 1, 1]) 9
layer1.2.conv2.weight torch.Size([64, 64, 3, 3]) 10
layer1.2.conv3.weight torch.Size([256, 64, 1, 1]) 11
layer2.0.conv1.weight torch.Size([128, 256, 1, 1]) 12
layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) 13
layer2.0.conv3.weight torch.Size([512, 128, 1, 1]) 14
layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1]) 15
layer2.1.conv1.weight torch.Size([128, 512, 1, 1]) 16
layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) 17
layer2.1.conv3.weight torch.Size([512, 128, 1, 1]) 18
layer2.2.conv1.weight torch.Size([128, 512, 1, 1]) 19
layer2.2.conv2.weight torch.Size([128, 128, 3, 3]) 20
layer2.2.conv3.weight torch.Size([512, 128, 1, 1]) 21
layer2.3.conv1.weight torch.Size([128, 512, 1, 1]) 22
layer2.3.conv2.weight torch.Size([128, 128, 3, 3]) 23
layer2.3.conv3.weight torch.Size([512, 128, 1, 1]) 24
layer3.0.conv1.weight torch.Size([256, 512, 1, 1]) 25
layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) 26
layer3.0.conv3.weight torch.Size([1024, 256, 1, 1]) 27
layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1]) 28
layer3.1.conv1.weight torch.Size([256, 1024, 1, 1]) 29
layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) 30
layer3.1.conv3.weight torch.Size([1024, 256, 1, 1]) 31
layer3.2.conv1.weight torch.Size([256, 1024, 1, 1]) 32
layer3.2.conv2.weight torch.Size([256, 256, 3, 3]) 33
layer3.2.conv3.weight torch.Size([1024, 256, 1, 1]) 34
layer3.3.conv1.weight torch.Size([256, 1024, 1, 1]) 35
layer3.3.conv2.weight torch.Size([256, 256, 3, 3]) 36
layer3.3.conv3.weight torch.Size([1024, 256, 1, 1]) 37
layer3.4.conv1.weight torch.Size([256, 1024, 1, 1]) 38
layer3.4.conv2.weight torch.Size([256, 256, 3, 3]) 39
layer3.4.conv3.weight torch.Size([1024, 256, 1, 1]) 40
layer3.5.conv1.weight torch.Size([256, 1024, 1, 1]) 41
layer3.5.conv2.weight torch.Size([256, 256, 3, 3]) 42
layer3.5.conv3.weight torch.Size([1024, 256, 1, 1]) 43
layer3.6.conv1.weight torch.Size([256, 1024, 1, 1]) 44
layer3.6.conv2.weight torch.Size([256, 256, 3, 3]) 45
layer3.6.conv3.weight torch.Size([1024, 256, 1, 1]) 46
layer3.7.conv1.weight torch.Size([256, 1024, 1, 1]) 47
layer3.7.conv2.weight torch.Size([256, 256, 3, 3]) 48
layer3.7.conv3.weight torch.Size([1024, 256, 1, 1]) 49
layer3.8.conv1.weight torch.Size([256, 1024, 1, 1]) 50
layer3.8.conv2.weight torch.Size([256, 256, 3, 3]) 51
layer3.8.conv3.weight torch.Size([1024, 256, 1, 1]) 52
layer3.9.conv1.weight torch.Size([256, 1024, 1, 1]) 53
layer3.9.conv2.weight torch.Size([256, 256, 3, 3]) 54
layer3.9.conv3.weight torch.Size([1024, 256, 1, 1]) 55
layer3.10.conv1.weight torch.Size([256, 1024, 1, 1]) 56
layer3.10.conv2.weight torch.Size([256, 256, 3, 3]) 57
layer3.10.conv3.weight torch.Size([1024, 256, 1, 1]) 58
layer3.11.conv1.weight torch.Size([256, 1024, 1, 1]) 59
layer3.11.conv2.weight torch.Size([256, 256, 3, 3]) 60
layer3.11.conv3.weight torch.Size([1024, 256, 1, 1]) 61
layer3.12.conv1.weight torch.Size([256, 1024, 1, 1]) 62
layer3.12.conv2.weight torch.Size([256, 256, 3, 3]) 63
layer3.12.conv3.weight torch.Size([1024, 256, 1, 1]) 64
layer3.13.conv1.weight torch.Size([256, 1024, 1, 1]) 65
layer3.13.conv2.weight torch.Size([256, 256, 3, 3]) 66
layer3.13.conv3.weight torch.Size([1024, 256, 1, 1]) 67
layer3.14.conv1.weight torch.Size([256, 1024, 1, 1]) 68
layer3.14.conv2.weight torch.Size([256, 256, 3, 3]) 69
layer3.14.conv3.weight torch.Size([1024, 256, 1, 1]) 70
layer3.15.conv1.weight torch.Size([256, 1024, 1, 1]) 71
layer3.15.conv2.weight torch.Size([256, 256, 3, 3]) 72
layer3.15.conv3.weight torch.Size([1024, 256, 1, 1]) 73
layer3.16.conv1.weight torch.Size([256, 1024, 1, 1]) 74
layer3.16.conv2.weight torch.Size([256, 256, 3, 3]) 75
layer3.16.conv3.weight torch.Size([1024, 256, 1, 1]) 76
layer3.17.conv1.weight torch.Size([256, 1024, 1, 1]) 77
layer3.17.conv2.weight torch.Size([256, 256, 3, 3]) 78
layer3.17.conv3.weight torch.Size([1024, 256, 1, 1]) 79
layer3.18.conv1.weight torch.Size([256, 1024, 1, 1]) 80
layer3.18.conv2.weight torch.Size([256, 256, 3, 3]) 81
layer3.18.conv3.weight torch.Size([1024, 256, 1, 1]) 82
layer3.19.conv1.weight torch.Size([256, 1024, 1, 1]) 83
layer3.19.conv2.weight torch.Size([256, 256, 3, 3]) 84
layer3.19.conv3.weight torch.Size([1024, 256, 1, 1]) 85
layer3.20.conv1.weight torch.Size([256, 1024, 1, 1]) 86
layer3.20.conv2.weight torch.Size([256, 256, 3, 3]) 87
layer3.20.conv3.weight torch.Size([1024, 256, 1, 1]) 88
layer3.21.conv1.weight torch.Size([256, 1024, 1, 1]) 89
layer3.21.conv2.weight torch.Size([256, 256, 3, 3]) 90
layer3.21.conv3.weight torch.Size([1024, 256, 1, 1]) 91
layer3.22.conv1.weight torch.Size([256, 1024, 1, 1]) 92
layer3.22.conv2.weight torch.Size([256, 256, 3, 3]) 93
layer3.22.conv3.weight torch.Size([1024, 256, 1, 1]) 94
layer4.0.conv1.weight torch.Size([512, 1024, 1, 1]) 95
layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) 96
layer4.0.conv3.weight torch.Size([2048, 512, 1, 1]) 97
layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1]) 98
layer4.1.conv1.weight torch.Size([512, 2048, 1, 1]) 99
layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) 100
layer4.1.conv3.weight torch.Size([2048, 512, 1, 1]) 101
layer4.2.conv1.weight torch.Size([512, 2048, 1, 1]) 102
layer4.2.conv2.weight torch.Size([512, 512, 3, 3]) 103
layer4.2.conv3.weight torch.Size([2048, 512, 1, 1]) 104
fc.weight torch.Size([1000, 2048]) 105</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>105</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>layer_name <span class="op">=</span> <span class="st">'layer3.9.conv3.weight'</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>with_bn, without_bn <span class="op">=</span> [], []</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>imgs, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet101()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">apply</span>(xavier_init)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bn, l <span class="kw">in</span> <span class="bu">zip</span>([<span class="va">True</span>, <span class="va">False</span>], [with_bn, without_bn]):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> bn: disable_bn(model)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr <span class="op">=</span> <span class="fl">0.1</span>, momentum <span class="op">=</span> <span class="fl">0.9</span>, weight_decay <span class="op">=</span> <span class="fl">5e-4</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    images, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(images)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(output, labels)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizer.step()</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> name <span class="op">==</span> layer_name:</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># l.append((param.grad ** 2).sum().item())</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(param.grad.shape)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            l.extend(param.grad.view(<span class="op">-</span><span class="dv">1</span>).cpu().detach().tolist())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1024, 256, 1, 1])
torch.Size([1024, 256, 1, 1])</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>f, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>sns.histplot(with_bn, ax <span class="op">=</span> ax1)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'With batch normalization'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>sns.histplot(without_bn, ax <span class="op">=</span> ax2)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">0.05</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Without batch normalization'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ax2.set_ylim((0, 2400))</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>f.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-18" class="cell" data-execution_count="194">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, p.data.size(), end <span class="op">=</span> <span class="st">' '</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(p.data.size()) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'yes n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>, end <span class="op">=</span> <span class="st">''</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>conv1.weight torch.Size([64, 3, 7, 7]) yes n = 1
bn1.weight torch.Size([64]) 
bn1.bias torch.Size([64]) 
layer1.0.conv1.weight torch.Size([64, 64, 1, 1]) yes n = 2
layer1.0.bn1.weight torch.Size([64]) 
layer1.0.bn1.bias torch.Size([64]) 
layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) yes n = 3
layer1.0.bn2.weight torch.Size([64]) 
layer1.0.bn2.bias torch.Size([64]) 
layer1.0.conv3.weight torch.Size([256, 64, 1, 1]) yes n = 4
layer1.0.bn3.weight torch.Size([256]) 
layer1.0.bn3.bias torch.Size([256]) 
layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1]) yes n = 5
layer1.0.downsample.1.weight torch.Size([256]) 
layer1.0.downsample.1.bias torch.Size([256]) 
layer1.1.conv1.weight torch.Size([64, 256, 1, 1]) yes n = 6
layer1.1.bn1.weight torch.Size([64]) 
layer1.1.bn1.bias torch.Size([64]) 
layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) yes n = 7
layer1.1.bn2.weight torch.Size([64]) 
layer1.1.bn2.bias torch.Size([64]) 
layer1.1.conv3.weight torch.Size([256, 64, 1, 1]) yes n = 8
layer1.1.bn3.weight torch.Size([256]) 
layer1.1.bn3.bias torch.Size([256]) 
layer1.2.conv1.weight torch.Size([64, 256, 1, 1]) yes n = 9
layer1.2.bn1.weight torch.Size([64]) 
layer1.2.bn1.bias torch.Size([64]) 
layer1.2.conv2.weight torch.Size([64, 64, 3, 3]) yes n = 10
layer1.2.bn2.weight torch.Size([64]) 
layer1.2.bn2.bias torch.Size([64]) 
layer1.2.conv3.weight torch.Size([256, 64, 1, 1]) yes n = 11
layer1.2.bn3.weight torch.Size([256]) 
layer1.2.bn3.bias torch.Size([256]) 
layer2.0.conv1.weight torch.Size([128, 256, 1, 1]) yes n = 12
layer2.0.bn1.weight torch.Size([128]) 
layer2.0.bn1.bias torch.Size([128]) 
layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) yes n = 13
layer2.0.bn2.weight torch.Size([128]) 
layer2.0.bn2.bias torch.Size([128]) 
layer2.0.conv3.weight torch.Size([512, 128, 1, 1]) yes n = 14
layer2.0.bn3.weight torch.Size([512]) 
layer2.0.bn3.bias torch.Size([512]) 
layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1]) yes n = 15
layer2.0.downsample.1.weight torch.Size([512]) 
layer2.0.downsample.1.bias torch.Size([512]) 
layer2.1.conv1.weight torch.Size([128, 512, 1, 1]) yes n = 16
layer2.1.bn1.weight torch.Size([128]) 
layer2.1.bn1.bias torch.Size([128]) 
layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) yes n = 17
layer2.1.bn2.weight torch.Size([128]) 
layer2.1.bn2.bias torch.Size([128]) 
layer2.1.conv3.weight torch.Size([512, 128, 1, 1]) yes n = 18
layer2.1.bn3.weight torch.Size([512]) 
layer2.1.bn3.bias torch.Size([512]) 
layer2.2.conv1.weight torch.Size([128, 512, 1, 1]) yes n = 19
layer2.2.bn1.weight torch.Size([128]) 
layer2.2.bn1.bias torch.Size([128]) 
layer2.2.conv2.weight torch.Size([128, 128, 3, 3]) yes n = 20
layer2.2.bn2.weight torch.Size([128]) 
layer2.2.bn2.bias torch.Size([128]) 
layer2.2.conv3.weight torch.Size([512, 128, 1, 1]) yes n = 21
layer2.2.bn3.weight torch.Size([512]) 
layer2.2.bn3.bias torch.Size([512]) 
layer2.3.conv1.weight torch.Size([128, 512, 1, 1]) yes n = 22
layer2.3.bn1.weight torch.Size([128]) 
layer2.3.bn1.bias torch.Size([128]) 
layer2.3.conv2.weight torch.Size([128, 128, 3, 3]) yes n = 23
layer2.3.bn2.weight torch.Size([128]) 
layer2.3.bn2.bias torch.Size([128]) 
layer2.3.conv3.weight torch.Size([512, 128, 1, 1]) yes n = 24
layer2.3.bn3.weight torch.Size([512]) 
layer2.3.bn3.bias torch.Size([512]) 
layer3.0.conv1.weight torch.Size([256, 512, 1, 1]) yes n = 25
layer3.0.bn1.weight torch.Size([256]) 
layer3.0.bn1.bias torch.Size([256]) 
layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 26
layer3.0.bn2.weight torch.Size([256]) 
layer3.0.bn2.bias torch.Size([256]) 
layer3.0.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 27
layer3.0.bn3.weight torch.Size([1024]) 
layer3.0.bn3.bias torch.Size([1024]) 
layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1]) yes n = 28
layer3.0.downsample.1.weight torch.Size([1024]) 
layer3.0.downsample.1.bias torch.Size([1024]) 
layer3.1.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 29
layer3.1.bn1.weight torch.Size([256]) 
layer3.1.bn1.bias torch.Size([256]) 
layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 30
layer3.1.bn2.weight torch.Size([256]) 
layer3.1.bn2.bias torch.Size([256]) 
layer3.1.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 31
layer3.1.bn3.weight torch.Size([1024]) 
layer3.1.bn3.bias torch.Size([1024]) 
layer3.2.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 32
layer3.2.bn1.weight torch.Size([256]) 
layer3.2.bn1.bias torch.Size([256]) 
layer3.2.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 33
layer3.2.bn2.weight torch.Size([256]) 
layer3.2.bn2.bias torch.Size([256]) 
layer3.2.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 34
layer3.2.bn3.weight torch.Size([1024]) 
layer3.2.bn3.bias torch.Size([1024]) 
layer3.3.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 35
layer3.3.bn1.weight torch.Size([256]) 
layer3.3.bn1.bias torch.Size([256]) 
layer3.3.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 36
layer3.3.bn2.weight torch.Size([256]) 
layer3.3.bn2.bias torch.Size([256]) 
layer3.3.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 37
layer3.3.bn3.weight torch.Size([1024]) 
layer3.3.bn3.bias torch.Size([1024]) 
layer3.4.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 38
layer3.4.bn1.weight torch.Size([256]) 
layer3.4.bn1.bias torch.Size([256]) 
layer3.4.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 39
layer3.4.bn2.weight torch.Size([256]) 
layer3.4.bn2.bias torch.Size([256]) 
layer3.4.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 40
layer3.4.bn3.weight torch.Size([1024]) 
layer3.4.bn3.bias torch.Size([1024]) 
layer3.5.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 41
layer3.5.bn1.weight torch.Size([256]) 
layer3.5.bn1.bias torch.Size([256]) 
layer3.5.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 42
layer3.5.bn2.weight torch.Size([256]) 
layer3.5.bn2.bias torch.Size([256]) 
layer3.5.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 43
layer3.5.bn3.weight torch.Size([1024]) 
layer3.5.bn3.bias torch.Size([1024]) 
layer3.6.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 44
layer3.6.bn1.weight torch.Size([256]) 
layer3.6.bn1.bias torch.Size([256]) 
layer3.6.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 45
layer3.6.bn2.weight torch.Size([256]) 
layer3.6.bn2.bias torch.Size([256]) 
layer3.6.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 46
layer3.6.bn3.weight torch.Size([1024]) 
layer3.6.bn3.bias torch.Size([1024]) 
layer3.7.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 47
layer3.7.bn1.weight torch.Size([256]) 
layer3.7.bn1.bias torch.Size([256]) 
layer3.7.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 48
layer3.7.bn2.weight torch.Size([256]) 
layer3.7.bn2.bias torch.Size([256]) 
layer3.7.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 49
layer3.7.bn3.weight torch.Size([1024]) 
layer3.7.bn3.bias torch.Size([1024]) 
layer3.8.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 50
layer3.8.bn1.weight torch.Size([256]) 
layer3.8.bn1.bias torch.Size([256]) 
layer3.8.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 51
layer3.8.bn2.weight torch.Size([256]) 
layer3.8.bn2.bias torch.Size([256]) 
layer3.8.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 52
layer3.8.bn3.weight torch.Size([1024]) 
layer3.8.bn3.bias torch.Size([1024]) 
layer3.9.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 53
layer3.9.bn1.weight torch.Size([256]) 
layer3.9.bn1.bias torch.Size([256]) 
layer3.9.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 54
layer3.9.bn2.weight torch.Size([256]) 
layer3.9.bn2.bias torch.Size([256]) 
layer3.9.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 55
layer3.9.bn3.weight torch.Size([1024]) 
layer3.9.bn3.bias torch.Size([1024]) 
layer3.10.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 56
layer3.10.bn1.weight torch.Size([256]) 
layer3.10.bn1.bias torch.Size([256]) 
layer3.10.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 57
layer3.10.bn2.weight torch.Size([256]) 
layer3.10.bn2.bias torch.Size([256]) 
layer3.10.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 58
layer3.10.bn3.weight torch.Size([1024]) 
layer3.10.bn3.bias torch.Size([1024]) 
layer3.11.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 59
layer3.11.bn1.weight torch.Size([256]) 
layer3.11.bn1.bias torch.Size([256]) 
layer3.11.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 60
layer3.11.bn2.weight torch.Size([256]) 
layer3.11.bn2.bias torch.Size([256]) 
layer3.11.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 61
layer3.11.bn3.weight torch.Size([1024]) 
layer3.11.bn3.bias torch.Size([1024]) 
layer3.12.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 62
layer3.12.bn1.weight torch.Size([256]) 
layer3.12.bn1.bias torch.Size([256]) 
layer3.12.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 63
layer3.12.bn2.weight torch.Size([256]) 
layer3.12.bn2.bias torch.Size([256]) 
layer3.12.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 64
layer3.12.bn3.weight torch.Size([1024]) 
layer3.12.bn3.bias torch.Size([1024]) 
layer3.13.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 65
layer3.13.bn1.weight torch.Size([256]) 
layer3.13.bn1.bias torch.Size([256]) 
layer3.13.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 66
layer3.13.bn2.weight torch.Size([256]) 
layer3.13.bn2.bias torch.Size([256]) 
layer3.13.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 67
layer3.13.bn3.weight torch.Size([1024]) 
layer3.13.bn3.bias torch.Size([1024]) 
layer3.14.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 68
layer3.14.bn1.weight torch.Size([256]) 
layer3.14.bn1.bias torch.Size([256]) 
layer3.14.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 69
layer3.14.bn2.weight torch.Size([256]) 
layer3.14.bn2.bias torch.Size([256]) 
layer3.14.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 70
layer3.14.bn3.weight torch.Size([1024]) 
layer3.14.bn3.bias torch.Size([1024]) 
layer3.15.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 71
layer3.15.bn1.weight torch.Size([256]) 
layer3.15.bn1.bias torch.Size([256]) 
layer3.15.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 72
layer3.15.bn2.weight torch.Size([256]) 
layer3.15.bn2.bias torch.Size([256]) 
layer3.15.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 73
layer3.15.bn3.weight torch.Size([1024]) 
layer3.15.bn3.bias torch.Size([1024]) 
layer3.16.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 74
layer3.16.bn1.weight torch.Size([256]) 
layer3.16.bn1.bias torch.Size([256]) 
layer3.16.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 75
layer3.16.bn2.weight torch.Size([256]) 
layer3.16.bn2.bias torch.Size([256]) 
layer3.16.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 76
layer3.16.bn3.weight torch.Size([1024]) 
layer3.16.bn3.bias torch.Size([1024]) 
layer3.17.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 77
layer3.17.bn1.weight torch.Size([256]) 
layer3.17.bn1.bias torch.Size([256]) 
layer3.17.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 78
layer3.17.bn2.weight torch.Size([256]) 
layer3.17.bn2.bias torch.Size([256]) 
layer3.17.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 79
layer3.17.bn3.weight torch.Size([1024]) 
layer3.17.bn3.bias torch.Size([1024]) 
layer3.18.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 80
layer3.18.bn1.weight torch.Size([256]) 
layer3.18.bn1.bias torch.Size([256]) 
layer3.18.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 81
layer3.18.bn2.weight torch.Size([256]) 
layer3.18.bn2.bias torch.Size([256]) 
layer3.18.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 82
layer3.18.bn3.weight torch.Size([1024]) 
layer3.18.bn3.bias torch.Size([1024]) 
layer3.19.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 83
layer3.19.bn1.weight torch.Size([256]) 
layer3.19.bn1.bias torch.Size([256]) 
layer3.19.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 84
layer3.19.bn2.weight torch.Size([256]) 
layer3.19.bn2.bias torch.Size([256]) 
layer3.19.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 85
layer3.19.bn3.weight torch.Size([1024]) 
layer3.19.bn3.bias torch.Size([1024]) 
layer3.20.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 86
layer3.20.bn1.weight torch.Size([256]) 
layer3.20.bn1.bias torch.Size([256]) 
layer3.20.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 87
layer3.20.bn2.weight torch.Size([256]) 
layer3.20.bn2.bias torch.Size([256]) 
layer3.20.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 88
layer3.20.bn3.weight torch.Size([1024]) 
layer3.20.bn3.bias torch.Size([1024]) 
layer3.21.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 89
layer3.21.bn1.weight torch.Size([256]) 
layer3.21.bn1.bias torch.Size([256]) 
layer3.21.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 90
layer3.21.bn2.weight torch.Size([256]) 
layer3.21.bn2.bias torch.Size([256]) 
layer3.21.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 91
layer3.21.bn3.weight torch.Size([1024]) 
layer3.21.bn3.bias torch.Size([1024]) 
layer3.22.conv1.weight torch.Size([256, 1024, 1, 1]) yes n = 92
layer3.22.bn1.weight torch.Size([256]) 
layer3.22.bn1.bias torch.Size([256]) 
layer3.22.conv2.weight torch.Size([256, 256, 3, 3]) yes n = 93
layer3.22.bn2.weight torch.Size([256]) 
layer3.22.bn2.bias torch.Size([256]) 
layer3.22.conv3.weight torch.Size([1024, 256, 1, 1]) yes n = 94
layer3.22.bn3.weight torch.Size([1024]) 
layer3.22.bn3.bias torch.Size([1024]) 
layer4.0.conv1.weight torch.Size([512, 1024, 1, 1]) yes n = 95
layer4.0.bn1.weight torch.Size([512]) 
layer4.0.bn1.bias torch.Size([512]) 
layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) yes n = 96
layer4.0.bn2.weight torch.Size([512]) 
layer4.0.bn2.bias torch.Size([512]) 
layer4.0.conv3.weight torch.Size([2048, 512, 1, 1]) yes n = 97
layer4.0.bn3.weight torch.Size([2048]) 
layer4.0.bn3.bias torch.Size([2048]) 
layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1]) yes n = 98
layer4.0.downsample.1.weight torch.Size([2048]) 
layer4.0.downsample.1.bias torch.Size([2048]) 
layer4.1.conv1.weight torch.Size([512, 2048, 1, 1]) yes n = 99
layer4.1.bn1.weight torch.Size([512]) 
layer4.1.bn1.bias torch.Size([512]) 
layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) yes n = 100
layer4.1.bn2.weight torch.Size([512]) 
layer4.1.bn2.bias torch.Size([512]) 
layer4.1.conv3.weight torch.Size([2048, 512, 1, 1]) yes n = 101
layer4.1.bn3.weight torch.Size([2048]) 
layer4.1.bn3.bias torch.Size([2048]) 
layer4.2.conv1.weight torch.Size([512, 2048, 1, 1]) yes n = 102
layer4.2.bn1.weight torch.Size([512]) 
layer4.2.bn1.bias torch.Size([512]) 
layer4.2.conv2.weight torch.Size([512, 512, 3, 3]) yes n = 103
layer4.2.bn2.weight torch.Size([512]) 
layer4.2.bn2.bias torch.Size([512]) 
layer4.2.conv3.weight torch.Size([2048, 512, 1, 1]) yes n = 104
layer4.2.bn3.weight torch.Size([2048]) 
layer4.2.bn3.bias torch.Size([2048]) 
fc.weight torch.Size([1000, 2048]) yes n = 105
fc.bias torch.Size([1000]) </code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="194">
<pre><code>105</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="189">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> model.parameters():</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i.data.size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([64, 3, 7, 7])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([64, 256, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([64, 256, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([128, 256, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 256, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([256, 512, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([1024, 512, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([512, 1024, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([2048, 1024, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([512, 2048, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([512, 2048, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([1000, 2048])
torch.Size([1000])</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="179">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, m <span class="kw">in</span> model.named_modules():</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m.requires_grad_ <span class="kw">and</span> m.data.size <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(name)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[179], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> n <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">0</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> name, m <span style="font-weight:bold;color:rgb(175,0,255)">in</span> model<span style="color:rgb(98,98,98)">.</span>named_modules():
<span class="ansi-green-fg">----&gt; 3</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> m<span style="color:rgb(98,98,98)">.</span>requires_grad_ <span style="font-weight:bold;color:rgb(175,0,255)">and</span> <span class="ansi-yellow-bg">m</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">data</span><span style="color:rgb(98,98,98)">.</span>size <span style="color:rgb(98,98,98)">&gt;</span> <span style="color:rgb(98,98,98)">1</span>:
<span class="ansi-green-fg ansi-bold">      4</span>         <span style="color:rgb(0,135,0)">print</span>(name)
<span class="ansi-green-fg ansi-bold">      5</span>         n <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">1</span>

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1695</span>, in <span class="ansi-cyan-fg">Module.__getattr__</span><span class="ansi-blue-fg">(self, name)</span>
<span class="ansi-green-fg ansi-bold">   1693</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> name <span style="font-weight:bold;color:rgb(175,0,255)">in</span> modules:
<span class="ansi-green-fg ansi-bold">   1694</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> modules[name]
<span class="ansi-green-fg">-&gt; 1695</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">AttributeError</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span><span style="color:rgb(0,135,0)">type</span>(<span style="color:rgb(0,135,0)">self</span>)<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__name__</span><span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)"> object has no attribute </span><span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>name<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-red-fg">AttributeError</span>: 'ResNet' object has no attribute 'data'</pre>
</div>
</div>
</div>
<div id="cell-21" class="cell" data-execution_count="140">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, m <span class="kw">in</span> model.named_children():</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>conv1
bn1
relu
maxpool
layer1
layer2
layer3
layer4
avgpool
fc</code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-execution_count="127">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>disable_bn(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-23" class="cell" data-execution_count="132">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>train(model, train_loader, val_loader, init_lr <span class="op">=</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/165 | Training loss: 0.0171 | Validation loss: 0.0134 | Validation accuracy: 0.4152
Epoch 3/165 | Training loss: 0.0123 | Validation loss: 0.0111 | Validation accuracy: 0.4976
Epoch 4/165 | Training loss: 0.0107 | Validation loss: 0.0106 | Validation accuracy: 0.5200</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[132], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">train</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">train_loader</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">val_loader</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">init_lr</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.1</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[130], line 58</span>, in <span class="ansi-cyan-fg">train</span><span class="ansi-blue-fg">(model, train, val, init_lr, plateau_patience)</span>
<span class="ansi-green-fg ansi-bold">     55</span> train_accs<span style="color:rgb(98,98,98)">.</span>append(acc)
<span class="ansi-green-fg ansi-bold">     57</span> <span style="font-style:italic;color:rgb(95,135,135)"># Evaluate the model on the validation set</span>
<span class="ansi-green-fg">---&gt; 58</span> val_loss, val_acc <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">eval_model</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">val</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">criterion</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     59</span> val_losses<span style="color:rgb(98,98,98)">.</span>append(val_loss)
<span class="ansi-green-fg ansi-bold">     60</span> val_accs<span style="color:rgb(98,98,98)">.</span>append(val_acc)

Cell <span class="ansi-green-fg">In[76], line 33</span>, in <span class="ansi-cyan-fg">eval_model</span><span class="ansi-blue-fg">(model, test, criterion)</span>
<span class="ansi-green-fg ansi-bold">     31</span> correct, loss <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">0.0</span>
<span class="ansi-green-fg ansi-bold">     32</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> torch<span style="color:rgb(98,98,98)">.</span>no_grad():
<span class="ansi-green-fg">---&gt; 33</span>     <span style="font-weight:bold;color:rgb(0,135,0)">for</span> images, labels <span style="font-weight:bold;color:rgb(175,0,255)">in</span> test:
<span class="ansi-green-fg ansi-bold">     34</span>         images, labels <span style="color:rgb(98,98,98)">=</span> images<span style="color:rgb(98,98,98)">.</span>to(device), labels<span style="color:rgb(98,98,98)">.</span>to(device)
<span class="ansi-green-fg ansi-bold">     35</span>         _, pred <span style="color:rgb(98,98,98)">=</span> torch<span style="color:rgb(98,98,98)">.</span>max(model(images), <span style="color:rgb(98,98,98)">1</span>)

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:630</span>, in <span class="ansi-cyan-fg">_BaseDataLoaderIter.__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    627</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_sampler_iter <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg ansi-bold">    628</span>     <span style="font-style:italic;color:rgb(95,135,135)"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="ansi-green-fg ansi-bold">    629</span>     <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_reset()  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[call-arg]</span>
<span class="ansi-green-fg">--&gt; 630</span> data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_next_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    631</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_num_yielded <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">1</span>
<span class="ansi-green-fg ansi-bold">    632</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_dataset_kind <span style="color:rgb(98,98,98)">==</span> _DatasetKind<span style="color:rgb(98,98,98)">.</span>Iterable <span style="font-weight:bold;color:rgb(175,0,255)">and</span> \
<span class="ansi-green-fg ansi-bold">    633</span>         <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_IterableDataset_len_called <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(175,0,255)">and</span> \
<span class="ansi-green-fg ansi-bold">    634</span>         <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_num_yielded <span style="color:rgb(98,98,98)">&gt;</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_IterableDataset_len_called:

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:674</span>, in <span class="ansi-cyan-fg">_SingleProcessDataLoaderIter._next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    672</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">_next_data</span>(<span style="color:rgb(0,135,0)">self</span>):
<span class="ansi-green-fg ansi-bold">    673</span>     index <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_next_index()  <span style="font-style:italic;color:rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 674</span>     data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_dataset_fetcher</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">fetch</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">index</span><span class="ansi-yellow-bg">)</span>  <span style="font-style:italic;color:rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-fg ansi-bold">    675</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_pin_memory:
<span class="ansi-green-fg ansi-bold">    676</span>         data <span style="color:rgb(98,98,98)">=</span> _utils<span style="color:rgb(98,98,98)">.</span>pin_memory<span style="color:rgb(98,98,98)">.</span>pin_memory(data, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_pin_memory_device)

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51</span>, in <span class="ansi-cyan-fg">_MapDatasetFetcher.fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-fg ansi-bold">     49</span>         data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>dataset<span style="color:rgb(98,98,98)">.</span>__getitems__(possibly_batched_index)
<span class="ansi-green-fg ansi-bold">     50</span>     <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">---&gt; 51</span>         data <span style="color:rgb(98,98,98)">=</span> [<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>dataset[idx] <span style="font-weight:bold;color:rgb(0,135,0)">for</span> idx <span style="font-weight:bold;color:rgb(175,0,255)">in</span> possibly_batched_index]
<span class="ansi-green-fg ansi-bold">     52</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">     53</span>     data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>dataset[possibly_batched_index]

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51</span>, in <span class="ansi-cyan-fg">&lt;listcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-fg ansi-bold">     49</span>         data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>dataset<span style="color:rgb(98,98,98)">.</span>__getitems__(possibly_batched_index)
<span class="ansi-green-fg ansi-bold">     50</span>     <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">---&gt; 51</span>         data <span style="color:rgb(98,98,98)">=</span> [<span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">dataset</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">idx</span><span class="ansi-yellow-bg">]</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> idx <span style="font-weight:bold;color:rgb(175,0,255)">in</span> possibly_batched_index]
<span class="ansi-green-fg ansi-bold">     52</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">     53</span>     data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>dataset[possibly_batched_index]

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torchvision/datasets/cifar.py:118</span>, in <span class="ansi-cyan-fg">CIFAR10.__getitem__</span><span class="ansi-blue-fg">(self, index)</span>
<span class="ansi-green-fg ansi-bold">    115</span> img <span style="color:rgb(98,98,98)">=</span> Image<span style="color:rgb(98,98,98)">.</span>fromarray(img)
<span class="ansi-green-fg ansi-bold">    117</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>transform <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg">--&gt; 118</span>     img <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">transform</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">img</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    120</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>target_transform <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg ansi-bold">    121</span>     target <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>target_transform(target)

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/transforms.py:95</span>, in <span class="ansi-cyan-fg">Compose.__call__</span><span class="ansi-blue-fg">(self, img)</span>
<span class="ansi-green-fg ansi-bold">     93</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">__call__</span>(<span style="color:rgb(0,135,0)">self</span>, img):
<span class="ansi-green-fg ansi-bold">     94</span>     <span style="font-weight:bold;color:rgb(0,135,0)">for</span> t <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>transforms:
<span class="ansi-green-fg">---&gt; 95</span>         img <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">t</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">img</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     96</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> img

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/transforms.py:137</span>, in <span class="ansi-cyan-fg">ToTensor.__call__</span><span class="ansi-blue-fg">(self, pic)</span>
<span class="ansi-green-fg ansi-bold">    129</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">__call__</span>(<span style="color:rgb(0,135,0)">self</span>, pic):
<span class="ansi-green-fg ansi-bold">    130</span> <span style="color:rgb(188,188,188)">    </span><span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">    131</span> <span style="font-style:italic;color:rgb(175,0,0)">    Args:</span>
<span class="ansi-green-fg ansi-bold">    132</span> <span style="font-style:italic;color:rgb(175,0,0)">        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    135</span> <span style="font-style:italic;color:rgb(175,0,0)">        Tensor: Converted image.</span>
<span class="ansi-green-fg ansi-bold">    136</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>
<span class="ansi-green-fg">--&gt; 137</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">F</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">to_tensor</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">pic</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/functional.py:174</span>, in <span class="ansi-cyan-fg">to_tensor</span><span class="ansi-blue-fg">(pic)</span>
<span class="ansi-green-fg ansi-bold">    172</span> img <span style="color:rgb(98,98,98)">=</span> img<span style="color:rgb(98,98,98)">.</span>permute((<span style="color:rgb(98,98,98)">2</span>, <span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">1</span>))<span style="color:rgb(98,98,98)">.</span>contiguous()
<span class="ansi-green-fg ansi-bold">    173</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(img, torch<span style="color:rgb(98,98,98)">.</span>ByteTensor):
<span class="ansi-green-fg">--&gt; 174</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">img</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">to</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">dtype</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">default_float_dtype</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)">.</span>div(<span style="color:rgb(98,98,98)">255</span>)
<span class="ansi-green-fg ansi-bold">    175</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">    176</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> img

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
<div id="cell-24" class="cell" data-execution_count="96">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>train(model, train_loader, val_loader, <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/165 | Training loss: 0.0170 | Validation loss: 0.0131 | Validation accuracy: 0.3928
Epoch 3/165 | Training loss: 0.0123 | Validation loss: 0.0109 | Validation accuracy: 0.4899</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[96], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">train</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">train_loader</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">val_loader</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.1</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[94], line 53</span>, in <span class="ansi-cyan-fg">train</span><span class="ansi-blue-fg">(model, train, val, init_lr, plateau_patience)</span>
<span class="ansi-green-fg ansi-bold">     50</span>         param_group[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">lr</span><span style="color:rgb(175,0,0)">'</span>] <span style="color:rgb(98,98,98)">=</span> lr_schedule[epoch]
<span class="ansi-green-fg ansi-bold">     52</span> <span style="font-style:italic;color:rgb(95,135,135)"># Train the model for an epoch</span>
<span class="ansi-green-fg">---&gt; 53</span> loss, acc <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">train_epoch</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">optimizer</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">criterion</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     54</span> train_losses<span style="color:rgb(98,98,98)">.</span>append(loss)
<span class="ansi-green-fg ansi-bold">     55</span> train_accs<span style="color:rgb(98,98,98)">.</span>append(acc)

Cell <span class="ansi-green-fg">In[94], line 19</span>, in <span class="ansi-cyan-fg">train_epoch</span><span class="ansi-blue-fg">(model, train, optimizer, criterion)</span>
<span class="ansi-green-fg ansi-bold">     17</span> loss<span style="color:rgb(98,98,98)">.</span>backward()
<span class="ansi-green-fg ansi-bold">     18</span> optimizer<span style="color:rgb(98,98,98)">.</span>step()
<span class="ansi-green-fg">---&gt; 19</span> train_loss <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">loss</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">item</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     20</span> _, pred <span style="color:rgb(98,98,98)">=</span> torch<span style="color:rgb(98,98,98)">.</span>max(output, <span style="color:rgb(98,98,98)">1</span>)
<span class="ansi-green-fg ansi-bold">     21</span> correct <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> (pred <span style="color:rgb(98,98,98)">==</span> labels)<span style="color:rgb(98,98,98)">.</span>float()<span style="color:rgb(98,98,98)">.</span>sum()<span style="color:rgb(98,98,98)">.</span>item()

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
</section>
<section id="what-i-learned" class="level3">
<h3 class="anchored" data-anchor-id="what-i-learned">What I learned</h3>
<ul>
<li>How to do image augmentation</li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>